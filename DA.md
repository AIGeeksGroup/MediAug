# Data Augmentation in General CV

  

> The core idea of data augmentation is to improve the sufficiency and diversity of training data by generating synthetic dataset. The challenge is that data augmentation methods are tasks-independent, because the operations are performed on the image data and labels at the same time, and the label types are different under different tasks, the data augmentation methods for object detection task can not be directly applied to semantic segmentation task.

  

## Part1- Basic Data Augmentation Methods

1. **Image Transformations** (image manipulation):
    1. [Flipping](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=Nalepa+J%2C+Marcinkiewicz+M%2C+Kawulok+M+%282019%29+Data+augmentation+for+brain-tumor+segmentation%3A+a+review.+Front+Comput+Neurosci+13%3A1%E2%80%9318&btnG=): Flip the image horizontally, vertically, or both.
    2. [Rotation](https://link.springer.com/article/10.1007/s10462-023-10453-z): Rotate the image at an angle.
    3. [Scaling Ratio](https://link.springer.com/article/10.1007/s10462-023-10453-z): Increase or reduce the image size.
    4. [Noise injection](https://ieeexplore.ieee.org/abstract/document/6985597): Add noise into the image.
    5. [Colour space](https://arxiv.org/abs/1405.3531): Change the image colour channels.
    6. [Contrast](https://opg.optica.org/josaa/abstract.cfm?uri=josaa-7-10-2032): Change the image contrast.
    7. [Sharpening](https://ieeexplore.ieee.org/abstract/document/4476197): Modify the image's sharpness.
    8. [Translation](https://link.springer.com/article/10.1186/s40537-019-0197-0): Move the image horizontally, vertically, or both.
    9. [Cropping](https://dl.acm.org/doi/abs/10.1145/3478513.3480566): Crop a sub-region of the image.
    10. [Shearing](https://link.springer.com/article/10.1007/s10462-023-10453-z): Skew the image along the horizontal or vertical axis to simulate perspective distortion.
    11. [Jitter](https://link.springer.com/chapter/10.1007/978-3-319-66787-4_3): Introduce subtle random variations in image properties such as brightness, contrast, saturation, or hue to enhance robustness.
    12. [Local Augment](https://openaccess.thecvf.com/content_CVPR_2019/html/Luo_ContextDesc_Local_Descriptor_Augmentation_With_Cross-Modality_Context_CVPR_2019_paper.html): Apply localized transformations to specific regions of the image to enhance diversity while preserving global structure.
    13. [Kernel Filters](https://ieeexplore.ieee.org/abstract/document/4767749/citations#citations): like Gaussian filters, enhance data augmentation by creating variations such as blurring and sharpening. In scale-space filtering, Gaussian kernels uniquely enable hierarchical signal analysis through consistent extrema behavior and zero-crossing contours.
    14. [Fixmatch](https://arxiv.org/abs/2001.07685): A streamlined yet highly effective semi-supervised learning (SSL) algorithm that utilizes high-confidence pseudo-labels derived from weakly-augmented images to guide training on their strongly-augmented counterparts. Despite its simplicity, FixMatch attains state-of-the-art performance across various SSL benchmarks, underscoring its efficacy even with extremely limited labeled data.
    15. [Intensities](https://link.springer.com/article/10.1007/s10462-023-10453-z): involves modifying image properties such as contrast, brightness, blurring, sharpening, or adding noise to enhance model robustness against variations in image quality. Different types of noise, such as Gaussian, salt-and-pepper, or uniform, are applied by altering pixel values through specific distribution-based sampling methods, simulating diverse imaging conditions.
2. **Image Erasing**: (delete one or more sub-regions in the image) The main idea is to replace the pixel values of these sub-regions with constant values or random values.
    1. [cutout](https://arxiv.org/abs/1708.04552): a simple regularization technique of randomly masking out square regions of input during training convolutional neural networks (CNNs)
    2. [Hide-and-Seek (HaS)](https://arxiv.org/abs/1811.02545): hide patches in a training image randomly to force the network to seek other relevant content while the most discriminative content is hidden
    3. [random erasing](https://ojs.aaai.org/index.php/AAAI/article/view/7000): selects a rectangle region in an image randomly and replaces its pixels with random values
    4. [GridMask](https://arxiv.org/abs/2001.04086): the deleted regions are a set of spatially uniformly distributed squares, which can be controlled in terms of density and size.
    5. [FenceMask](https://arxiv.org/abs/2006.07877): based on the simulation of object occlusion strategy to balance the object occlusion and information retention
3. **Image Mix**: completed by mixing two or more images or sub-regions of images into one
    1. [pairing samples](https://arxiv.org/abs/1801.02929): enlarge the dataset by synthesizing every new image with two images randomly selected in the training set, which can average the intensity of two images on each pixel
    2. [Mixup](https://arxiv.org/abs/1710.09412): do linear interpolation between two different images to create a new training sample[](https://github.com/demoleiwang/awesome-mixup)
    3. [CutMix](https://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html): replaces the removed regions with a patch from another image and can generate more natural images compared to Mixup [https://github.com/clovaai/CutMix-PyTorch](https://github.com/clovaai/CutMix-PyTorch)
    4. [CropMix](https://arxiv.org/abs/2205.15955): Sampling a Rich Input Distribution via Multi-Scale Cropping [https://github.com/JunlinHan/CropMix](https://github.com/JunlinHan/CropMix)
    5. [YOCO](https://arxiv.org/abs/2201.12078): You Only Cut Once: Boosting Data Augmentation with a Single Cut [https://github.com/JunlinHan/YOCO](https://github.com/JunlinHan/YOCO)
    6. [Fmix](https://arxiv.org/abs/2002.12047): uses random binary masks obtained by applying a threshold to low-frequency images sampled from Fourier space, which can take on a wide range of shapes of random masks and can improve performance over Mixup and CutMix
    2. [AugMix](https://arxiv.org/abs/1912.02781): first mixes multiple augmentation operations into three augmentation chains and then mixes together the results of several augmentation chains in convex combinations, so the whole process is typically mixing the results generated by the same image in different augmentation pipelines [https://github.com/google-research/augmix](https://github.com/google-research/augmix)
    3. [ManifoldMix](https://proceedings.mlr.press/v97/verma19a.html): improve the hidden representations and decision boundaries of neural networks at multiple layers by mixing hidden representations rather than input samples
    4. [Self-augmentation](https://arxiv.org/abs/2004.00251): addresses the challenges of few-shot learning by combining self-mix, a regional dropout technique that substitutes image patches with other values within the same image, and self-distillation, which enforces knowledge sharing across auxiliary branches of a backbone network. This approach enhances generalization by preventing overfitting to dataset-specific structures and enabling diverse learning, achieving state-of-the-art performance on few-shot benchmarks.
    5. [SalfMix](https://www.mdpi.com/1424-8220/21/24/8444): a novel data augmentation method that generates self-mixed images using saliency maps, effectively improving generalization by applying mixing strategies to a single image. Combined with state-of-the-art two-image methods in HybridMix, SalfMix achieves superior performance on classification and object detection tasks, outperforming existing approaches like Cutout and achieving state-of-the-art results.
    6. [Cut-Thumbnail](https://arxiv.org/abs/2103.05342): a data augmentation method that replaces part of an image with its reduced thumbnail, enhancing shape bias and preserving global information. It outperforms state-of-the-art methods in classification and detection tasks, achieving significant accuracy improvements.
    7. [SaliencyMix](https://arxiv.org/abs/2006.01791): a data augmentation method that uses saliency maps to select informative image patches, mixing them with target images to guide models toward learning meaningful feature representations. It outperforms existing approaches in ImageNet classification, enhances robustness against adversarial attacks, and improves object detection performance.
    8. [Puzzle Mix](https://arxiv.org/abs/2009.06962): a mixup-based data augmentation method that leverages saliency information and underlying data statistics to create more informative virtual examples while avoiding misleading supervisory signals. It achieves state-of-the-art generalization and adversarial robustness on benchmarks like CIFAR-100, Tiny-ImageNet, and ImageNet.
    9. [SnapMix](https://arxiv.org/abs/2012.04846): a data augmentation method that uses class activation maps (CAM) to reduce label noise by generating target labels based on the semantic composition of mixed images. It outperforms existing mix-based approaches in fine-grained recognition, ensuring semantic consistency and achieving top-level performance across various datasets and network depths. [https://github.com/Shaoli-Huang/SnapMix](https://github.com/Shaoli-Huang/SnapMix)
    10. [MixMo](https://arxiv.org/abs/2103.06132): a framework for training multi-input multi-output deep subnetworks, replacing suboptimal summing operations with feature-level binary mixing inspired by data augmentations like CutMix. It achieves state-of-the-art results in image classification on CIFAR-100 and Tiny ImageNet, outperforming augmented ensembles without additional inference or memory overhead.
    11. [StyleMix](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_StyleMix_Separating_Content_and_Style_for_Enhanced_Data_Augmentation_CVPR_2021_paper.html): the first mixup method to separately manipulate content and style features of image pairs, creating more diverse and robust samples to improve model generalization. It achieves competitive performance with state-of-the-art mixup methods and enhances robustness against adversarial attacks on datasets like CIFAR-100, CIFAR-10, and ImageNet.
    12. [RandomMix](https://arxiv.org/abs/2311.03629): a novel family of local transformations based on Gaussian random fields, significantly expanding the augmentation space for self-supervised representation learning by allowing pixel-wise variation in transformation parameters. Empirical results demonstrate its effectiveness, achieving notable improvements in downstream classification tasks on ImageNet and iNaturalist datasets.
    13. [MixMatch](https://proceedings.neurips.cc/paper_files/paper/2019/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html): unifies key semi-supervised learning approaches by guessing low-entropy labels for augmented unlabeled data and mixing labeled and unlabeled data with MixUp. It achieves state-of-the-art results, significantly reducing error rates on datasets like CIFAR-10 and STL-10.
    14. [ReMixMatch](https://arxiv.org/abs/1911.09785): improves MixMatch by introducing distribution alignment and augmentation anchoring, enhancing the use of unlabeled data through better prediction consistency and learned augmentation policies. It achieves state-of-the-art data efficiency, requiring significantly fewer labeled examples to match or surpass prior results on benchmarks like CIFAR-10.
    15. [Copy-Paste](https://openaccess.thecvf.com/content/CVPR2021/html/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.html?ref=https://githubhelp.com): a data augmentation technique for instance segmentation that randomly pastes objects onto images, providing significant performance gains even without modeling visual context. It achieves state-of-the-art results on COCO and LVIS benchmarks, particularly improving performance on rare object categories.
    16. [Mixed-Example](https://ieeexplore.ieee.org/abstract/document/8659168): explores a generalized form of mixed-example data augmentation, extending beyond linear combinations of examples to uncover a broader range of effective augmentation techniques. This approach not only improves performance over previous methods but also challenges existing theories, highlighting the need for a more comprehensive understanding of such augmentations.
    17. [RICAP](https://proceedings.mlr.press/v95/takahashi18a.html?ref=https://githubhelp.com): (Random Image Cropping and Patching) is a data augmentation technique that creates new training images by randomly cropping and patching four images while mixing their class labels, enhancing data variety and regularization. It achieves state-of-the-art performance on benchmarks like CIFAR-10, CIFAR-100, and ImageNet, outperforming other augmentation methods such as cutout and mixup.
    18. [CutBlur](https://arxiv.org/abs/2004.00448): a data augmentation method designed for low-level vision tasks, such as super-resolution, by cutting and pasting low-resolution patches onto high-resolution images and vice versa. This approach helps models learn "how," "where," and "how much" to super-resolve, significantly improving performance across various scenarios and other tasks like denoising and artifact removal.
    19. [ResizeMix](https://arxiv.org/abs/2012.11101): a simple yet effective data augmentation method that improves upon CutMix by resizing a source image into a small patch and pasting it onto another image, preserving more object information. It outperforms CutMix and saliency-guided methods in image classification and object detection tasks, achieving superior results without additional computational cost.
    20. [ClassMix](https://arxiv.org/abs/2007.07936): a novel data augmentation technique for semi-supervised semantic segmentation that mixes unlabeled samples by leveraging network predictions to respect object boundaries. It achieves state-of-the-art results on standard benchmarks, addressing the limitations of traditional augmentations in segmentation tasks.
    21. [CDA](https://arxiv.org/abs/2204.07674): Contrastive Data Augmentation (CDA) is a specialized data augmentation strategy designed to enhance contrastive learning by generating semantically consistent positive pairs and diverse negative pairs. By leveraging techniques such as random cropping, color jittering, and strong augmentations, CDA aims to improve the quality of feature representations and boost performance in tasks like knowledge distillation and self-supervised learning.
    22. [ObjectAug](https://arxiv.org/abs/2102.00221): an object-level data augmentation method for semantic segmentation that decouples images into objects and backgrounds, applies augmentations to each object, and restores artifacts via inpainting. This approach enhances boundary diversity and supports category-aware augmentation, improving segmentation performance.

  

## Part2- Advanced Approaches

1. **Auto Augment:** automatically search augmentation approaches to obtain improved performance
    1. [AutoAugment](https://openaccess.thecvf.com/content_CVPR_2019/html/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html): automatically search for improved data augmentation policies. It consists of two parts: search algorithm and search space. The search algorithm is designed to find the best policy with the highest validation accuracy. The search space contains many policies which detail various augmentation operations and magnitudes with which the operations are applied.
    2. [Fast AutoAugment](https://arxiv.org/abs/1905.00397): finds effective augmentation policies via a more efficient search strategy based on density matching which can speed up the search time
    3. [Population Based Augmentation (PBA)](https://proceedings.mlr.press/v97/ho19b.html): reduce the time cost of AutoAugment which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. PBA can match the performance of AutoAugment on multiple datasets with less computation time.
    4. [RandAugment](https://arxiv.org/abs/1909.13719): dramatically reduces the search space for data augmentation by removing a separate search, which is computationally expensive. In addition, it also improves the performance of AutoAugment and PBA.
    5. [KeepAugment](https://arxiv.org/abs/2011.11778): use the saliency map to detect important regions on the original images and then preserve these informative regions during augmentation
    6. [OHL-Auto-Aug](https://arxiv.org/abs/1905.07373): an augmentation policy as a parameterized probability distribution and the parameters can be optimized jointly with network parameters
    7. [Augmentation-wise Weight Sharing](https://arxiv.org/abs/2009.14737): improves efficiency significantly and makes it affordable to directly search on large-scale datasets
    8. [RAD (Reinforcement Learning with Augmented Data)](https://arxiv.org/abs/2004.14990): is a versatile module that enhances RL algorithms by incorporating general data augmentations, improving both data efficiency and generalization to new environments. RAD achieves state-of-the-art performance on benchmarks like the DeepMind Control Suite and OpenAI Gym, while also demonstrating superior test-time generalization on OpenAI ProcGen tasks. [https://github.com/MishaLaskin/rad](https://github.com/MishaLaskin/rad)
    9. [MARL (Multi-Agent Reinforcement Learning)](https://ieeexplore.ieee.org/document/10611035): is a powerful framework for controlling multi-robot systems but suffers from low sample efficiency, limiting its practical applications. To address this, adaptive methods like AdaptAUG have been proposed to optimize data augmentation strategies, significantly improving sample efficiency and performance in both simulated and real-world multi-robot tasks.
    10. [Scale-Aware Automatic Augmentation](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Scale-Aware_Automatic_Augmentation_for_Object_Detection_CVPR_2021_paper.html?ref=https://githubhelp.com): introduces a novel scale-aware search space and a Pareto Scale Balance metric to efficiently learn augmentation policies tailored for object detection, ensuring scale invariance at both image and box levels. This approach delivers significant performance improvements across various object detectors and tasks like instance segmentation and keypoint estimation, while maintaining lower search costs compared to previous methods.
    11. [ADA (Adaptive Data Augmentation)](https://arxiv.org/abs/2405.11467): introduces a reinforcement learning-based framework to dynamically adjust augmentation magnitudes for individual training samples, addressing the misalignment between static augmentations and the evolving training status of models. By leveraging a dual-model architecture, AdaAugment achieves superior generalization performance across various benchmarks, consistently outperforming existing data augmentation methods in both effectiveness and efficiency.
    12. [RADA (Robust Adversarial Data Augmentation)](https://ieeexplore.ieee.org/document/10341653): introduces a targeted approach to data augmentation by perturbing the most vulnerable pixels, enhancing robustness in camera localization under challenging conditions. This method significantly outperforms existing techniques, achieving up to double the accuracy of state-of-the-art models, even in unseen adverse weather scenarios.
    13. [PTDA (Perspective Transformation Data Augmentation)](https://ieeexplore.ieee.org/document/8943416): introduces a novel framework that generates annotated data by simulating images from various camera viewpoints, effectively expanding limited datasets without additional manual labelling. This approach enhances the performance of deep CNNs, particularly on small or imbalanced datasets, as demonstrated by extensive experiments across multiple benchmarks.
    14. [DADA (Differentiable Automatic Data Augmentation)](https://arxiv.org/abs/2003.03780): introduces a novel approach to data augmentation policy optimization by formulating it as a differentiable problem using Gumbel-Softmax and an unbiased gradient estimator, RELAX. This method significantly accelerates the search process, achieving state-of-the-art efficiency and comparable accuracy across multiple datasets, while also demonstrating its value for pre-training in downstream tasks.
2. **Feature Augmentation:** performs the transformation in a learned feature space, rather than conducting augmentation only in the input space
    1. [FeatMatch](https://arxiv.org/abs/2007.08505): is a novel learned feature-based refinement and augmentation method to produce a varied set of complex transformations. It also can utilize information from both within-class and across-class prototypical representations.
    2. [Moment Exchange](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_On_Feature_Normalization_and_Data_Augmentation_CVPR_2021_paper.pdf): encouraging the models to utilize the moment information of latent features. Specifically, the moments of the learned features of one training image are replaced by those of another.
    3. [Dataset Augmentation in Feature Space](https://arxiv.org/abs/1702.05538): introduces a domain-agnostic approach to augment training data by applying simple transformations, such as noise addition and interpolation, directly in a learned feature space rather than the input space. This method is effective for both static and sequential data, leveraging unsupervised representation learning to enhance generalization across diverse tasks.
    4. [Feature Space Augmentation for Long-Tailed Data](https://link.springer.com/chapter/10.1007/978-3-030-58526-6_41): addresses class imbalance by generating novel samples for under-represented classes in the feature space, leveraging class-generic and class-specific components extracted via class activation maps. This approach achieves state-of-the-art performance across multiple long-tailed datasets, effectively enhancing representation for minority classes.
    5. [Adversarial Feature Augmentation for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content_cvpr_2018/html/Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper.html): introduces a novel approach that utilizes a GAN-based framework to perform feature augmentation, enhancing representations for under-represented classes while enforcing domain invariance. This method achieves superior or comparable results to state-of-the-art techniques across various unsupervised domain adaptation benchmarks.
    6. [LDAS (Latent Data Augmentation Strategy)](https://link.springer.com/article/10.1007/s13349-023-00705-5): introduces a novel approach for generating additional training data in structural health monitoring (SHM) by leveraging a conditional variational autoencoder (CVAE) to model and augment the statistical distributions of power cepstral coefficients under various damage conditions. This method enhances the performance and robustness of damage classification tasks, as demonstrated through numerical simulations and experimental validations.
    7. [STaDA (Style Transfer as Data Augmentation)](https://arxiv.org/abs/1909.01056): leverages neural style transfer techniques to enrich training datasets by applying artistic styles to images while preserving their semantic content, enhancing variation for image classification tasks. Experimental results on Caltech 101 and Caltech 256 demonstrate that STaDA, combined with traditional augmentation methods, improves classification accuracy and reduces reliance on large labeled datasets.
    8. [NSTDA (Non-Statistical Targeted Data Augmentation)](https://www.sciencedirect.com/science/article/pii/S2590005622000911): addresses the challenge of insufficient training data by employing advanced augmentation techniques that enhance the volume, quality, and diversity of datasets in a task-specific manner. This approach integrates modern methods such as neural rendering, 3D graphics modeling, and generative adversarial networks, demonstrating improved performance across various computer vision tasks and datasets.
    9. [SAS (Self-Augmentation Strategy)](https://arxiv.org/abs/2106.07176): is a novel approach for pre-training language models that utilizes a single network to perform both regular pre-training and contextualized data augmentation, eliminating the need for a separate generator and reducing computational complexity. By jointly optimizing Masked Language Modeling (MLM) and Replaced Token Detection (RTD) tasks, SAS simplifies the training paradigm, addresses the challenge of balancing generator-discriminator capacities, and achieves state-of-the-art performance on GLUE benchmarks with comparable or reduced computational costs. [https://github.com/alibaba/self-augmentation-strategy](https://github.com/alibaba/self-augmentation-strategy)
3. **Deep Generative Models:** the data distribution we generate data from should not be different from the original one
    1. [Generative adversarial networks (GANs)](https://arxiv.org/pdf/1406.2661.pdf): The generator can help generate new images. The discriminator ensures that the gap between the newly generated images and the original images is not too large.
    2. [Pix2Pix](https://ieeexplore.ieee.org/document/8100115): It's used to learn the mapping from the input images to output images. However, a large amount of paired data is needed to train Pix2Pix. It is challenging to collect the paired data.
    3. [CycleGAN](https://ieeexplore.ieee.org/document/8237506): It is used to learn the translation of an image from the source domain X to a target domain Y in the absence of paired samples.
    4. [StarGAN](https://arxiv.org/abs/1711.09020): StarGAN builds only one model to perform image-to-image translation among multiple domains. In the generation phase, we just need to provide the generator with the source image and an attribute label which indicates the target domain.
    5. [StarGAN v2](https://arxiv.org/abs/1912.01865): To solve the problem that the same output per each domain given an input image, StarGAN v2 can generate diverse images across multiple domains, which means it can translate an image of one domain to diverse images of a target domain, and support multiple domains. [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)
    6. [GAN based augmentation](https://link.springer.com/article/10.1007/s10462-023-10453-z): leverages Generative Adversarial Networks (GANs) to create realistic synthetic images by training a generator and discriminator in tandem, enabling diverse and high-quality data augmentation. Advanced GAN variants, such as Conditional GAN, Cycle GAN, and Style GAN, enhance image generation by incorporating class labels, domain translation, and fine-grained detail synthesis, addressing challenges like quality and training stability.
    7. [Channel-wise gamma correction](https://link.springer.com/chapter/10.1007/978-3-030-87000-3_20): is a data augmentation technique that applies random gamma correction independently to each color channel of a fundus image, enhancing variability in color intensity. This method helps models learn more robust and invariant features, improving performance against global disturbances in retinal vessel segmentation tasks.
    8. [Zooming and CLAHE](https://ieeexplore.ieee.org/abstract/document/9332019): augmentation are effective techniques for enhancing retinal fundus image analysis in diabetic retinopathy classification. While random zooming introduces variability by simulating different focal levels, CLAHE enhances image contrast, together improving model accuracy, sensitivity, and specificity in deep learning applications.
    9. [Blurring and shifting](https://onlinelibrary.wiley.com/doi/full/10.1155/2021/6013448): augmentation are data enhancement technique used to improve the robustness of deep learning models in diabetic retinopathy classification. Random weak Gaussian blurring reduces image details to simulate real-world noise, while random shifting alters spatial positioning, collectively aiding in training models to handle spatial and visual variability.
    10. [Heuristic augmentation with NV-like structures](https://ieeexplore.ieee.org/abstract/document/9214404): is a data augmentation technique designed to address the scarcity of proliferative diabetic retinopathy (PDR) cases in retinal image datasets. By synthesizing neovessel-like structures based on their common locations and shapes, this method enhances dataset variability and improves the ability of deep learning models to detect PDR-related features.
    11. [Deep convolutional GAN](https://ieeexplore.ieee.org/abstract/document/9225684): leverages generative adversarial networks to synthesize diverse and realistic images for underrepresented classes, such as proliferative diabetic retinopathy, in imbalanced datasets. This approach enhances data availability and improves classification performance by providing high-quality synthetic samples without affecting other class distributions.
    12. [Conditional GAN](https://ieeexplore.ieee.org/abstract/document/9296950): utilizes generative adversarial networks to synthesize high-resolution fundus images conditioned on grading severity and lesion information, enabling targeted data generation for diabetic retinopathy classification. By incorporating structural masks and adaptive grading vectors, this approach enhances data diversity and improves model performance on grading and lesion segmentation tasks.
    13. [Style based GAN](https://ieeexplore.ieee.org/abstract/document/9288197): leverages generative adversarial networks to synthesize high-quality, class-specific images by learning and transferring fine-grained style features from real images. This approach addresses data scarcity in medical imaging, particularly for rare disease conditions, and enhances classification performance in diabetic retinopathy detection tasks.
    14. [StyPath(Style Transfer Pathway)](https://arxiv.org/abs/2007.05008): is a histological data augmentation technique designed to address variability in tissue stain quality, a common challenge in kidney transplant pathology for Antibody Mediated Rejection (AMR) classification. By leveraging a lightweight style-transfer algorithm, StyPath reduces sample-specific bias, improves classification performance, enhances model generalization, and demonstrates faster processing compared to other augmentation methods, as validated through Bayesian performance estimates and expert qualitative analysis.
    15. [Deep CNN Ensemble](https://arxiv.org/abs/1506.07224): is a variant of the R-CNN model that achieves state-of-the-art performance in object detection by combining complementary deep CNN architectures into an ensemble. By augmenting the PASCAL VOC training set with a curated subset of Microsoft COCO images, the method significantly improves training data diversity and achieves superior results on the PASCAL VOC 2012 detection task.